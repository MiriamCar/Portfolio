---
title: "Risk factors for cardiovascular heart disease"
author: "Miriam Cardozo"
date: "2023-03-07"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: spacelab
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Early detection of cardiovascular disease (CVD) in patients would favor
not only early treatment of symptoms but also could mean the difference
between life and death for thousands of people. Hence, machine learning
models may be very helpful in this matter.

In this data analytic project, machine learning algorithms are used in
order to predict the occurrence of cardiovascular disease conditions and
to identify the most important risk factors. For this purpose, we
evaluate de performance of different classification algorithms (logistic
regression, decision tree, random forest and k-nearest neighbor) and
optimize the final model in order to obtain the model with the greatest
predictive potential.

For the analysis we considered 10 variables height, weight, age, blood
pressure (systolic and diastolic), cholesterol and glucose blood levels,
as well as daily habits associated with smoking, physical activity and
alcohol consumption.

| Variable name    | Description                                                                                                                  |
|------------------|------------------------------------------------------------------------------------------------------------------------------|
| age              | Age of participant                                                                                                           |
| gender           | Gender of participant                                                                                                        |
| height           | Height measured in centimeters                                                                                               |
| weight           | Weight measured in kilograms                                                                                                 |
| ap_hi            | Systolic blood pressure reading taken from patient                                                                           |
| ap_lo            | Diastolic blood pressure reading taken from patient                                                                          |
| cholesterol      | Total cholesterol level read as mg/dl on a scale 0 - 5 units. Each unit denoting increase/decrease by 20 mg/dL respectively. |
| gluc             | Glucose level read as mmol/l on a scale 0 - 16+ units. Each unit denoting increase/ decrease by 1 mmol/L respectively.       |
| smoke            | Whether person smokes or not (0= No; 1= Yes)                                                                                 |
| active           | Whether person is physically active or not (0= No; 1= Yes)                                                                   |
| alco             | Whether person consumes alcoholor not (0= No; 1= Yes)                                                                        |
| cardio (outcome) | Whether person suffers from cardiovascular disease or not (0= No; 1= Yes)                                                    |

**Dataset information**

Kuzak Dempsy (2021) Risk Factors for Cardiovascular Heart Disease.
[Dataset available at:
[Kaggle](https://www.kaggle.com/datasets/thedevastator/exploring-risk-factors-for-cardiovascular-diseas))

## 1. Import dataset (.csv)

```{r}
datos <-read.csv("heart_data.csv", header=T, sep = ",", dec=".")
```

**Load libraries**

```{r message=FALSE}
library(dplyr) 
library(ggplot2)
library(caret)
library(ggpubr)
library(randomForest) 
library(tibble) 
library(recipes) 
library(rpart.plot)
library(ROSE)
```

## 2. Data Cleaning & Exploratory Data Analysis

After loading the data we have to look for missing values, outliers,
duplicate data and check if the variables have been stored with the type
of data that corresponds to its nature (numeric, character or factor)

### 2.a. Structure of data

```{r}
dim(datos) 
head(datos) 
str(datos) 
summary(datos)
```

Observations:

-   The dataset contains 70000 instances and 14 variables.

-   All variables are stored as numeric.

-   The variable "age" is measured in days.

### 2.b. Missing data and duplicated rows

```{r}
any(!complete.cases(datos)) 
anyDuplicated(datos)
```

There are no missing data nor duplicated rows in the dataset.

### 2.c. Response variable distribution

```{r}
#Response variable: cardio
#Factor conversion 
datos$cardio<- as.factor(datos$cardio)

#Check class balance 
table(datos$cardio) 
prop.table(table(datos$cardio))%>% round(digits=2)
```

Classes are balanced, 50% of the pacients have CVD and 50% don't have
the disease.

```{r}
#Plot distribution
ggplot(data = datos, aes(x = cardio, y = after_stat(count), 
                         fill = cardio)) + 
  geom_bar() + 
  scale_fill_manual(values = c("gray50", "orangered2")) + 
  labs(title = "Cardiovascular disease occurrence") + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

For a predictive model to be useful, it must have a success rate greater
than what is expected by chance or at a certain basal level. In
classification problems, the basal level is the one obtained if all
observations are assigned to the mode (class majority). In this dataset,
since 50% of people do not have CVD, if the model always predicts the
mode (Cardio = 0), the percentage of hits will be about 50%. This is the
minimum percentage that we have to try to exceed with the predictive
models (in the training set).

### 2.d.Quantitative variables

```{r}
#Remove from the dataset the variables "Index" and "Id" since they won't be used for building the models
datos<- datos[,c(-1,-2)] 
```

```{r}
#Transform "age" variable from days to years 
datos$age <- (datos$age/360)
datos$age <- as.integer(datos$age)
```

**Data visualization: Univariate plots**

```{r}
 par(mfrow=c(2,4)) 
 #Histograms 
 for (i in 1:8) {
   hist(datos [,i], main=names(datos)[i]) 
 } 
 #Boxplots
 for (i in 1:8) { 
   boxplot(datos [,i], main=names(datos)[i]) 
   }
```

```{r}
#Density matrix 
 x <- datos[, 1:8] 
 y <- datos[,12] #cardio 
scales <- list(x=list(relation="free"), y= list(relation="free"))
featurePlot(x=x, y=y, plot="box", scales=scales)

```

```{r}
#check mean values of predictors per class
datos %>% 
  group_by(cardio) %>% 
  summarise(mean_age= mean(age),
            mean_height=mean(height), 
            mean_weight= mean(weight),
            mean_aplo=mean(ap_lo),
            mean_aphi=mean(ap_hi), 
            mean_chol= mean(cholesterol), 
            mean_glucose=mean(gluc))
```

Observations:

-   The variables "ap_lo" and "ap_hi" show an exponential distribution
    and they take negative values and zeros. Since the blood pressure
    measurement cannot give zeros or negatives, we proceed to remove the
    corrupted data.

-   The variables "height", "weight", "ap_lo" and "ap_hi" have outliers

-   The variables "cholesterol" and "gluc" only take on three values (1,
    2 or 3), one alternative is to categorize them.

```{r message=FALSE}
#Remove values of ap_lo and ap_hi that are <=0 
# select the number of rows with values <=0 using the which function
#datos [ which (datos$ ap_hi <=0),] 
#datos [which (datos$ap_lo <= 0), ]

#remove the rows from the dataset 
datos <- datos [c(-4608, -16022, -20537, -23989, -25241, -35041, -46628, -2015, -13490, -16460, -17382, -22924, -23868, -25455, -27687, -31784, -38371, -40331, -41506, -42398, -43923, -45836, -48050, -52581, -56951, -60107, -63788, -65304, -68664),]
```

```{r}
##Outliers 
#filter outliers 
outlier_he<- boxplot(datos$height,data = datos, plot=FALSE)$out 
outlier_we<- boxplot(datos$weight,data = datos, plot=FALSE)$out 
outlier_aplo<- boxplot(datos$ap_lo,data = datos, plot=FALSE)$out 
outlier_aphi<- boxplot(datos$ap_hi,data = datos, plot=FALSE)$out 

#Remove outliers 
datos <- datos[!((datos$height %in% outlier_he) |  (datos$weight %in% outlier_we) | (datos$ap_lo %in% outlier_aplo) |  (datos$ap_hi %in% outlier_aphi)),] 
```

**Data visualization: Bivariate plots**

```{r}
#log(ap_hi) 
p1 <- ggplot(data = datos, aes(x = log(ap_hi), fill = cardio)) +
  geom_density(alpha = 0.5) + scale_fill_manual(values = c("gray50", "orangered2")) + 
  geom_rug(aes(color = cardio), alpha = 0.5) +
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw() 

p2 <- ggplot(data = datos, aes(x = cardio, y = log(ap_hi), color = cardio)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.15) + 
  scale_color_manual(values = c("gray50", "orangered2")) +
  theme_bw() 

final_plot1 <- ggarrange(p1, p2, legend = "top") 
final_plot1 <- annotate_figure(final_plot1, top = text_grob("Systolic blood pressure", size = 15)) 
final_plot1

#ap_lo 
p3 <- ggplot(data = datos, aes(x = log(ap_lo), fill = cardio)) +
  geom_density(alpha = 0.5) + 
  scale_fill_manual(values = c("gray50", "orangered2")) + 
  geom_rug(aes(color = cardio), alpha = 0.5) + 
  scale_color_manual(values = c("gray50", "orangered2")) +
  theme_bw()

p4 <- ggplot(data = datos, aes(x = cardio, y = log(ap_lo), color = cardio)) +
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw() 

final_plot2 <- ggarrange(p3, p4, legend = "top") 
final_plot2 <- annotate_figure(final_plot2, top = text_grob("Diastolic blood preasure", size = 15))
final_plot2

#age 
p5 <- ggplot(data = datos, aes(x = age, fill = cardio)) + 
  geom_density(alpha = 0.5) + scale_fill_manual(values = c("gray50", "orangered2")) +
  geom_rug(aes(color = cardio), alpha = 0.5) + 
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw()
p6 <- ggplot(data = datos, aes(x = cardio, y = age, color = cardio)) +
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw() 

final_plot3 <- ggarrange(p5, p6, legend = "top") 
final_plot3 <- annotate_figure(final_plot3, top = text_grob("Age", size = 15)) 
final_plot3

#cholesterol
p7 <- ggplot(data = datos, aes(x = cholesterol, fill = cardio)) + 
  geom_density(alpha = 0.5) + scale_fill_manual(values = c("gray50", "orangered2")) +
  geom_rug(aes(color = cardio), alpha = 0.5) + 
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw()
p8 <- ggplot(data = datos, aes(x = cardio, y = cholesterol, color = cardio)) +
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw() 

final_plot4 <- ggarrange(p7, p8, legend = "top") 
final_plot4 <- annotate_figure(final_plot4, top = text_grob("Cholesterol blood level", size = 15)) 
final_plot4


#glucose
p3 <- ggplot(data = datos, aes(x = gluc, fill = cardio)) + 
  geom_density(alpha = 0.5) + scale_fill_manual(values = c("gray50", "orangered2")) +
  geom_rug(aes(color = cardio), alpha = 0.5) + 
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw()
p4 <- ggplot(data = datos, aes(x = cardio, y = gluc, color = cardio)) + geom_boxplot(outlier.shape = NA) + 
  geom_jitter(alpha = 0.3, width = 0.15) +
  scale_color_manual(values = c("gray50", "orangered2")) + 
  theme_bw() 

final_plot5 <- ggarrange(p3, p4, legend = "top") 
final_plot5 <- annotate_figure(final_plot5, top = text_grob("Glucose blood level", size = 15)) 
final_plot5
```

**Observations:**

-   CVD condition appears mostly in patients with higher blood pressure
    levels (both diastolic and systolic).

-   Age seems to be an important factor in the occurrence of this
    cardiac condition, since most patients older than 55 years suffer
    from CVD.

### 2.e. Categorical variables

**Variable conversion**

The type of data of the variables "smoke", "alco", "active" and "gender"
does not correspond to the nature of the variable. Although these
variables are coded as 1 and 0, it should not be stored in numeric
format. To avoid this type of problem, the variables are categorized so
that their two possible levels are "Yes"-"No", "male" - "female".

```{r}
datos$smoke <- if_else(datos$smoke == 1, "Yes", "No") 
datos$smoke <- as.factor(datos$smoke)

datos$alco <- if_else(datos$alco == 1, "Yes", "No") 
datos$alco <- as.factor(datos$alco)

datos$active <- if_else(datos$active == 1, "Yes", "No") 
datos$active <- as.factor(datos$active)

datos$gender <- if_else(datos$gender == 2, "female", "male")
datos$gender <- as.factor(datos$gender)
```

**Data visualization: Bivariate plots**

```{r}
#gender 
ggplot(data = datos, aes(x = gender, y = after_stat(count), fill = cardio)) + 
  geom_bar() + 
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Gender") + 
  theme_bw() +
  theme(legend.position = "bottom") 
# Relative frequency table 
table(datos$gender, datos$cardio)
prop.table(table(datos$gender, datos$cardio), margin = 1) %>% round(digits = 2)

#smoke
ggplot(data = datos, aes(x = smoke, y = after_stat(count), fill = cardio)) + 
  geom_bar() + 
  scale_fill_manual(values = c("gray50", "orangered2")) + 
  labs(title = "Smoking habits") + 
  theme_bw() +
  theme(legend.position = "bottom")
# Relative frequency table 
table(datos$smoke, datos$cardio)
prop.table(table(datos$smoke, datos$cardio), margin = 1) %>% round(digits = 2)

#active
ggplot(data = datos, aes(x = active, y = after_stat(count), fill = cardio)) + geom_bar() + 
  scale_fill_manual(values = c("gray50", "orangered2")) + 
  labs(title = "Active") + 
  theme_bw() +
  theme(legend.position = "bottom")
# Relative frequency table 
table(datos$active, datos$cardio)
prop.table(table(datos$active, datos$cardio), margin = 1) %>% round(digits = 2)

#alcoholism
ggplot(data = datos, aes(x = alco, y = after_stat(count), fill = cardio)) + geom_bar() + 
  scale_fill_manual(values = c("gray50", "orangered2")) + 
  labs(title = "Alcohol consumer") + 
  theme_bw() +
  theme(legend.position = "bottom") 
# Relative frequency table 
table(datos$alco, datos$cardio)
prop.table(table(datos$alco, datos$cardio), margin = 1) %>% round(digits = 2)
```

**Observations:**

-   For the variables "alco", "smoke" and "active" classes are
    unbalanced.

### 2.f. Variable importance and selection

**Correlation analysis**

```{r}
#Highy correlated variables 
corMatrix<- cor(datos[,3:8]) 
print(corMatrix) 
highlyCorrelated<- findCorrelation(corMatrix, cutoff=0.70, names=TRUE) 

print(paste("variables with correlation coefficient > 0.70 =", highlyCorrelated))
```

**Variable Importance with Random Forest**

The random forest algorithm can be used for variable selection. This
algorithm does not require standardization, it handles quantitative and
categorical predictors and it is not influenced by outliers. However, it
is sensitive to unbalanced data and NAs.

```{r}
modelo_randforest <- randomForest(formula = cardio ~ . ,
                                  data = datos,
                                  mtry = 5,
                                  importance = TRUE,
                                  ntree = 500)

importancia <- as.data.frame(modelo_randforest$importance) 

importancia <- rownames_to_column(importancia,var = "variable")

#plot importance
p1.1 <- ggplot(data = importancia, aes(x = reorder(variable, MeanDecreaseAccuracy),y = MeanDecreaseAccuracy, fill = MeanDecreaseAccuracy)) +
  labs(x = "variable", title = "Accuracy reduction") +
  geom_col() +
  coord_flip() + 
  theme_bw() + 
  theme(legend.position = "bottom")

p2.2 <- ggplot(data = importancia, aes(x = reorder(variable, MeanDecreaseGini), y = MeanDecreaseGini, fill = MeanDecreaseGini)) + labs(x = "variable", title = "Gini reduction") +
  geom_col() + 
  coord_flip() +
  theme_bw() + 
  theme(legend.position = "bottom") 

ggarrange(p1.1, p2.2)
```

**Exploratory data analysis conclusions**

-   The exploration of the data seem to indicate that the factors that
    most influence the occurrence of cardiovascular disease (CVD) are:
    systolic and diastolic blood pressure, age, cholesterol blood level
    , weight and height of the patient.

-   A high correlation was also detected between the variables "ap_lo"
    and "ap_hi".

-   Considering the selection of variables carried out using the random
    forest algorithm for the construction of the predictive model, the
    following predictors will be used: "age", "gender", "ap_hi",
    "height", "weight", "glucose" and "cholesterol".

## 3. Splitting data

Create training and testing datasets, preserving the 50/50 class split
in each.

```{r}
set.seed(16) 
#Create a list of 80% of the instances for training 
validationIndex <-createDataPartition(datos$cardio, p=0.80, list=FALSE) #Use the remaining 20% to validate 
validation <- datos[-validationIndex,] 
training<- datos[validationIndex,]

#check the balance of the "cardio" classes in both sets
prop.table(table(validation$cardio)) 
prop.table(table(training$cardio))
```

## 4. Data preprocess

In order to prepare the data for modeling we use the library(recipes)
for standardization of numerical variables and binarization of factors.
Binarization consists of creating new dummy variables with each of the
levels of the qualitative variables. This process is also known as one
hot encoding.

```{r message=FALSE}
#Create an object that contains the response variable and the predictors of interest 
objeto_recipe <- recipe(formula = cardio ~ age + height + weight + 
                          ap_hi + cholesterol +gluc + gender, 
                        data = training) 
objeto_recipe

#Define the preprocessing steps 
objeto_recipe <- objeto_recipe %>% step_center (all_numeric())
objeto_recipe <- objeto_recipe %>% step_scale (all_numeric())
objeto_recipe <- objeto_recipe %>% step_dummy (all_nominal(),-all_outcomes())

#Trained the object with the training dataset
trained_recipe <- prep(objeto_recipe, training=training)

#Applied the preprocess to both training and validation dataset
p_training <- bake(trained_recipe, new_data = training)
p_validation <-bake(trained_recipe, new_data = validation)
```

## 5. Classification Methods

**Resampling method**

```{r}
 trainControl<- trainControl(method="cv", number=5) 
 metric<- "Accuracy"
```

### 5.a. Logistic Regression -GLM

```{r}
set.seed(7)
fit.glm<- train(cardio~., 
                data=p_training,
                method="glm", 
                metric=metric, 
                trControl=trainControl) 
summary(fit.glm)
```

### 5.b. Decision Tree - CART

```{r}
 set.seed(7) 
 fit.cart <- train(cardio~., 
                   data= p_training, 
                   method="rpart", 
                   metric=metric,
                   trControl=trainControl) 
#Plot decision tree
decision_tree_plot<- rpart.plot(fit.cart$finalModel)
```

### 5.c. Random Forest - RF

```{r}
set.seed(7) 
fit.rf <- train(cardio~., 
                data= p_training, 
                method="rf", 
                metric=metric, 
                trControl=trainControl)
```

### 5.d. KNN- K- nearest neighbour

```{r}
set.seed(7) 
fit.knn <- train(cardio~.,
                 data= p_training,
                 method="knn", 
                 metric=metric, 
                 trControl=trainControl)
```

## 6. Model evaluation

```{r}
results<- resamples(list(GLM=fit.glm, 
                         CART=fit.cart,
                         knn=fit.knn, 
                         RF=fit.rf)) 
summary(results) 

#Dotplot of the results
dotplot(results)
```

**Model evaluation conclusions**

-   All the algorithms managed to correctly classify more than 50% of
    the observations, thus exceeding the basal level.

-   Random forest algorithm showed the best performance with a mean
    accuracy of % 72.57 along with the logistic regression (mean
    accuracy of % 72.05).

-   Let's see if we can optimize the random forest algorithm.

## 7. Random forest optimization of hyperparameters

Random forest algorithm has two hyperparameters *ntree* and *mtry* that
can be optimized. The optimal value of *mtry* usually found is the
square root of N, where N is the total number of samples. In the model
`fit.rf` with an optimal value of *mtry* = 2 the model reached an
accuracy of % 72.57. So we will tune the *ntree* parameter and see if we
can optimize the model.

**Optimization of hyperparameters (mtry and ntree) with extend caret**

```{r}
#Custom random forest algorithm
customRF <- list(type = "Classification", library = "randomForest", 
                 loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), 
                                  class = rep("numeric", 2), 
                                  label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes 

#train model with different values of hyperparameter (ntree)
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=2, .ntree=seq(100,500, by=100))
set.seed(7)
custom <- train(cardio~., data=p_training, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)

#compare results
custom$results
plot(custom)
```

**Observations:**

-   The most accurate value for *ntree* is 500.

-   The final model with *ntree*= 500 and *mtry*= 2 reached a mean
    accuracy of 72.58% (a lift over our first model `fit.rf` using the
    default *mtry* value).

## 8. Final model predictions

```{r}
#Predictions
predict_finalmodel<- predict(custom, newdata = p_validation)

#Confusion Matriz
confusionMatrix(predict_finalmodel, p_validation$cardio)

#ROC curve plot
p_validation$cardio <- ifelse(p_validation$cardio=="0",0,1)
roc_rf <-roc.curve(p_validation$cardio,predict_finalmodel)

```

## **9. Final Conclusions**

-   Based on the statistics describing the performance of the models, we
    can conclude that the best classification model for *cardiovascular
    disease dataset* is random forest with a tuning hyperparameter of
    *mtry=2* and *ntree* = 500.

-   The final model predict correctly % 72.58 of observations and does
    not show overfitting.

-   The most importat risk factor associated with CVD are systolic blood
    pressure (ap_hi), age and cholesterol blood level.

-   According to the logistic regression coefficients for every one unit
    increase in systolic blood pressure, a person's risk of developing
    CVD triples. In addition, for each increase in age and cholesterol
    blood level (increase by 20 mg/dL), the risk increases by 40%.

**Some other considerations:**

-   A possible alternative to model the information provided by the
    variables ap_hi and ap_lo without removing one of them from the
    modeling (due to high correlation) is to create a new variable e.g.
    blood pressure, considering the 5 levels proposed by the [American
    Heart
    Association](https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings).
-   Since "cholesterol" and "glucose" only take on 3 numeric values, on
    other alternative is to categorize them.
-   Other classification algorithms than can be evaluated for this
    analysis are: Super Vector Machine as well as ensemble algorithms
    (using bagging or boosting methods).

## 10. References

KUHN, Max, et al. *Applied predictive modeling*. New York: Springer,
2013.

JAMES, Gareth, et al. *An introduction to statistical learning*. New
York: springer, 2013.

Machine Learning con R y caret by JoaquÃ­n Amat Rodrigo, available under
a Attribution 4.0 International (CC BY 4.0) at this
[link](https://www.cienciadedatos.net/documentos/41_machine_learning_con_r_y_caret)

Predicting patient's survival from heart failure and identify most
important risk factors by Andrea Furmanek and Divij Pherwani, available
at this [link](https://rpubs.com/USL/798455)
